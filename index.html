<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="description" content="Generalized Attention-based Task-synergized Estimation in 3D" />
    <meta name="keywords" content="Mono3D, GATE3D, CVPR, CV4MR" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>GATE3D: Generalized Attention-based Task-synergized Estimation in 3D</title>

    <!-- Fonts & Styles -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./static/css/index.css" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
  </head>

  <body>
    <!-- Hero section -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <!-- Title -->
              <h1 class="title is-1 publication-title">
                GATE3D: Generalized Attention-based Task-synergized Estimation in 3D
              </h1>

              <!-- Authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block">Eunsoo Im<sup>1</sup>,</span>
                <a href="https://www.linkedin.com/in/eunsoo-lim-0411/" target="_blank" rel="noopener noreferrer">
                  Eunsoo Im
                </a>
                <!-- <sup>1</sup> -->
                <!-- <span class="author-block">Changhyun Jee<sup>1</sup></span> -->
                <!-- <span class="author-block">Jung Kwon Lee<sup>1</sup>,</span> -->
              </div>
              <div class="is-size-6 publication-authors">
                <span class="author-block">Superb AI</span>
              </div>
              <div class="is-size-6 has-text-centered">
                <p>eslim@superb-ai.com</p>
              </div>


              <!-- <div class="is-size-5 has-text-centered publication-authors">
                <a href="https://www.linkedin.com/in/eunsoo-lim-0411/" target="_blank" rel="noopener noreferrer">
                  Eunsoo Im
                </a><sup>1</sup>,
                <span class="author-block">
                  Changhyun Jee<sup>1,2</sup>
                </span>,
                <span class="author-block">
                  Jung Kwon Lee<sup>1,2</sup>
                </span>
              </div>

              <div class="is-size-6 publication-authors">
                <span class="author-block">
                  <sup>1</sup>Superb AI, Korea
                </span>
                <span class="author-block">
                  <sup>2</sup>Added as co-authors in arXiv after camera-ready accepted to the CV4MR at CVPR'25
                </span>
              </div> -->

              <!-- 줄인 로고 크기 -->
              <div class="has-text-centered" style="margin-top: 1rem">
                <a href="https://www.superb-ai.com" target="_blank">
                  <img src="./static/images/CI_roundshape.png" alt="Superb AI Logo" style="max-width: 80px" />
                </a>
              </div>

              <!-- Buttons (ArXiv only for now) -->
              <div class="column has-text-centered" style="margin-top: 1rem">
                <div class="buttons is-centered"></div>
              </div>

              <div class="column has-text-centered" style="margin-top: 1rem">
                <p class="is-size-6" style="margin-bottom: 1rem">
                  <em>Presented at CVPR 2025 Workshop on Computer Vision for Mixed Reality (CV4MR)</em>
                </p>
                <div class="buttons is-centered">
                  <a href="https://arxiv.org/pdf/2504.11014" class="button is-dark is-rounded" target="_blank">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Abstract -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The emerging trend in computer vision emphasizes developing universal models capable of simultaneously
                addressing multiple diverse tasks. Such universality typically requires joint training across
                multi-domain datasets to ensure effective generalization. However, monocular 3D object detection
                presents unique challenges in multi-domain training due to the scarcity of datasets annotated with
                accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts.
                To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels.
                Current pretrained models often struggle to accurately detect pedestrians in non-road environments due
                to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar
                generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a
                novel framework designed specifically for generalized monocular 3D object detection via weak
                supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D
                predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on
                an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework.
                Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data
                through effective pre-training strategies, highlighting substantial potential for broader impacts in
                robotics, augmented reality, and virtual reality applications.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <h3 class="title is-3 has-text-centered">Qualitative Results</h3>
      <h6 class="title is-6 has-text-centered">Unseen Images</h6>
      <video autoplay loop muted playsinline style="max-height: 400px; width: auto; display: block; margin: 0 auto">
        <source src="./static/videos/5.mp4" type="video/mp4" />
      </video>

      <video autoplay loop muted playsinline style="max-height: 400px; width: auto; display: block; margin: 0 auto">
        <source src="./static/videos/2.mp4" type="video/mp4" />
      </video>

      <video autoplay loop muted playsinline style="max-height: 400px; width: auto; display: block; margin: 0 auto">
        <source src="./static/videos/3.mp4" type="video/mp4" />
      </video>
      <video autoplay loop muted playsinline style="max-height: 400px; width: auto; display: block; margin: 0 auto">
        <source src="./static/videos/4.mp4" type="video/mp4" />
      </video>
        <source src="./static/videos/6.mp4" type="video/mp4" />
      </video>

    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <h3 class="title is-3 has-text-centered">
          Metric-scale fidelity in a novel environment
        </h3>
        <video
          autoplay
          loop
          muted
          playsinline
          style="max-height: 400px; width: auto; display: block; margin: 0 auto"
        >
          <source src="./static/videos/home_export.mp4" type="video/mp4" />
        </video>

        <!-- === 여기에 플롯 이미지 추가 === -->
        <figure class="image" style="max-width: 600px; margin: 1.5rem auto;">
          <img
            src="./static/images/height_plot.png"
            alt="Estimated heights over 300 frames in the novel sequence"
          />
          <figcaption class="has-text-centered is-size-7">
            The qualitative snapshots and the quantitative height trace jointly demonstrate that
            <em>GATE3D</em> retains physically consistent scale when applied to an unseen indoor scene,
            confirming its reliability for downstream mixed-reality applications. Per-frame height estimates
            for the same sequence indicate a ground-truth stature of <strong>1.73m</strong>, whereas
            <em>GATE3D</em> yields a mean of <strong>1.718m</strong>, a median of
            <strong>1.718 m</strong>, and a variance of <code>7.06&times; 10<sup>-3</sup>m<sup>2</sup></code>
            (σ≈5.18cm) across 300 frames.
          </figcaption>
        </figure>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- 제목은 columns 바깥! -->
        <h3 class="title is-3 has-text-centered">overview</h3>
        <!-- 이미지 1 -->
        <div class="has-text-centered" style="margin-bottom: 2rem">
          <figure class="image">
            <img src="./static/images/overview.png" alt="Scene A" style="max-width: 800px; margin: 0 auto" />
          </figure>
          <!-- <p class="is-size-6">overview</p> -->
        </div>
        <div class="content has-text-justified">
          <p>
            GATE3D architecture overview. The proposed framework incorporates a DETR-style 3D detection backbone enhanced with
            attention-based modules, and supports both fully and weakly supervised learning modes. For ground-truth-labeled samples, the model is
            trained via standard 3D detection losses. For weakly labeled data, pseudo-3D annotations are generated from 2D detection, monocular
            depth estimation, and orientation prediction. To mitigate label noise, we introduce a 2D–3D consistency loss that aligns projected 3D box
            dimensions with frozen 2D predictions. Notably, during weak supervision, the 2D detector remains fixed to preserve its reliability, while
            only the 3D decoder is optimized. This hybrid learning strategy improves robustness and generalization across diverse domains.

          </p>
        </div>
      </div>

    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- 제목은 columns 바깥! -->
        <h3 class="title is-3 has-text-centered">Experiments</h3>
        <h6 class="title is-6 has-text-centered">KITTI Benchmark</h6>
        <!-- 이미지 1 -->
        <div class="has-text-centered" style="margin-bottom: 2rem">
          <figure class="image">
            <img src="./static/images/ex_car.png" alt="Scene A" style="max-width: 500px; margin: 0 auto" />
          </figure>
          <!-- <p class="is-size-6">Car</p> -->
        </div>

        <!-- 이미지 2 -->
        <div class="has-text-centered">
          <figure class="image">
            <img src="./static/images/ex_ped.png" alt="Scene B" style="max-width: 500px; margin: 0 auto" />
          </figure>
          <!-- <p class="is-size-6">Pedestrian</p> -->
        </div>
      </div>
      <div class="has-text-centered">
        <figure class="image">
          <img src="./static/images/car.png" alt="Scene B" style="max-width: 500px; margin: 0 auto" />
        </figure>
        <!-- <p class="is-size-6">Pedestrian</p> -->
      </div>
    </div>
    <div class="has-text-centered">
      <figure class="image">
        <img src="./static/images/ped.png" alt="Scene B" style="max-width: 500px; margin: 0 auto" />
      </figure>
      <!-- <p class="is-size-6">Pedestrian</p> -->
    </div>
  </div>

        <!-- 이미지 1 -->
        <!-- <div class="has-text-centered" style="margin-bottom: 2rem">
          <figure class="image">
            <img src="./static/images/car.png" alt="Scene A" style="max-width: 500px; margin: 0 auto" />
          </figure>
          <p class="is-size-6">Car</p>
        </div> -->

        <!-- 이미지 2 -->
        <!-- <div class="has-text-centered">
          <figure class="image">
            <img src="./static/images/ped.png" alt="Scene B" style="max-width: 500px; margin: 0 auto" />
          </figure>
          <p class="is-size-6">Pedestrian</p>
        </div>
      </div> -->
      <h6 class="title is-6 has-text-centered">office dataset</h6>
        <!-- 이미지 1 -->
        <div class="has-text-centered" style="margin-bottom: 2rem">
          <figure class="image">
            <img src="./static/images/ex_office.png" alt="Scene A" style="max-width: 500px; margin: 0 auto" />
          </figure>
          <!-- <p class="is-size-6">Person in Office</p> -->
        </div>


      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <!-- 제목은 columns 바깥! -->
        <h3 class="title is-3 has-text-centered">BibTeX</h3>
        <!-- 이미지 1 -->
        <div class="content has-text-justified">
          <pre><code>@article{im2025gate3d,
      title={GATE3D: Generalized Attention-based Task-synergized Estimation in 3D},
      author={Im, Eunsoo and Lee, Jung Kwon and Jee, Changhyun},
      journal={arXiv preprint arXiv:2504.11014},
      year={2025}
    }</code></pre>
        </div>
      </div>
    </section>
    <!-- BibTeX -->
  </body>
</html>
